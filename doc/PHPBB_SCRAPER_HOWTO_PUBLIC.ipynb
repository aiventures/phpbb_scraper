{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code for phpBB Scraper <a name=\"00\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [(01) Testing Query Parameter Generation](#01)\n",
    "* [(02) Create different phpBB URLs](#02)\n",
    "* [(03) Test reading different html files](#03)\n",
    "* [(04) Testing Request Module](#04)\n",
    "* [(05) Testing Web Scraping](#05)\n",
    "* [(06) Read json file and transform to dictionary](#06)\n",
    "* [(07) Create HTML Table  and save it to file](#07)\n",
    "* [(08) Read HTML file as soup, create filename, save as json, read json as dictionary, convert to HTML table and save HTML file](#08)\n",
    "* [(09) Read Credentials and URL Base from Config file, initialize session and scraper, read one page, save data](#09)\n",
    "* [(10) Read HTML file names from directory, read single html file](#10)\n",
    "* [(11) Generate Query URL, create URL iterator to call URLs with indices](#11)\n",
    "* [(12) Read Configuration, execute query, copy result to soup, replace relative by absolute links in soup](#12)\n",
    "* [(13) Create Filename, Save soup as data string](#13)\n",
    "* [(14) Convert soup as json entries, save it to file](#14)\n",
    "* [(15) Read Snippet for Number of Topics in Soup](#15)\n",
    "* [(16) Read JSON file, convert to dictionary, export as HTML file](#16)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (01) Testing Query Parameter Generation <a name=\"01\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phpbb_scraper import persistence\n",
    "\n",
    "# read user, password, url base from json file\n",
    "config_file = r\"C:\\<path_to>\\config.json\"\n",
    "config = persistence.read_json(filename=config_file)\n",
    "user = config['user']\n",
    "password = config['password']\n",
    "base = config['base']\n",
    "#print(f\"base {base} user {user} password {password} base {base}\")\n",
    "debug = False\n",
    "wait_time = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (02) Create different phpBB URLs <a name=\"02\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "from phpbb_scraper import url_generator\n",
    "from phpbb_scraper.url_generator import PhpbbUrlGenerator\n",
    "\n",
    "dir(PhpbbUrlGenerator)\n",
    "imp.reload(url_generator)\n",
    "dir(PhpbbUrlGenerator)\n",
    "\n",
    "url_gen = PhpbbUrlGenerator(base=base,debug=True)\n",
    "\n",
    "# Hint Check Variable definition in source code\n",
    "sort_by_creation_date = url_gen.get_query_param('SORT_BY_CREATION_DATE')\n",
    "sort_by_forum = url_gen.get_query_param('SORT_BY_FORUM')\n",
    "sort_descending = url_gen.get_query_param('SORT_DESCENDING')\n",
    "sort_by_rank = url_gen.get_query_param('SORT_BY_RANK')\n",
    "display_posts = url_gen.get_query_param('DISPLAY_POSTS')\n",
    "\n",
    "print(\"Possible Query Parameters and url params\",url_gen.QUERY_PARAMS_DICT)\n",
    "print(\"----------------------\")\n",
    "\n",
    "start = 20\n",
    "query = \"test\"\n",
    "\n",
    "search_posts_desc_bydate = url_gen.get_query_url(query=query,sort_by=sort_by_creation_date,\n",
    "                                             sort_order=sort_descending,search_result=display_posts,start=20)\n",
    "print(\"search descending by date, posts\",search_posts_desc_bydate)\n",
    "display_topics = url_gen.get_query_param('DISPLAY_TOPICS')\n",
    "# only search in title\n",
    "search_type = url_gen.get_query_param('SEARCH_TITLE')\n",
    "search_topics_desc_bydate = url_gen.get_query_url(query=query,sort_by=sort_by_forum,\n",
    "                                             sort_order=sort_descending,search_type=search_type,\n",
    "                                             search_result=display_topics,start=20)\n",
    "print(\"search descending by date, topics\",search_topics_desc_bydate)\n",
    "\n",
    "# url for member list\n",
    "member_list = url_gen.get_query_url(start=200,sort_by=sort_by_rank,member_list=True)\n",
    "print(\"member list:\",member_list)\n",
    "\n",
    "# url for latest topics\n",
    "active_topics =  url_gen.get_query_url(start=90,active_topics=True)\n",
    "print(\"active topics:\",active_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (03) Test reading different html files <a name=\"03\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "from phpbb_scraper import persistence\n",
    "from phpbb_scraper import soup_converter\n",
    "from phpbb_scraper import url_generator\n",
    "from phpbb_scraper.url_generator import PhpbbUrlGenerator\n",
    "\n",
    "imp.reload(url_generator)\n",
    "imp.reload(persistence)\n",
    "imp.reload(soup_converter)\n",
    "\n",
    "file_test = r\"C:\\C:\\<path_to>\\test.txt\"\n",
    "file_test_copy = r\"C:\\<path_to>\\test_copy.txt\"\n",
    "file_phpbb_topics = r\"C:\\<path_to>\\PAGE_TopicsOnly.html\"\n",
    "file_phpbb_posts = r\"C:\\<path_to>\\PAGE_TopicsAndTexts.html\"\n",
    "file_phpbb_team_member = r\"C:\\<path_to>\\PAGE_TeamMembers.html\"\n",
    "file_phpbb_member_profile = r\"C:\\<path_to>\\PAGE_MemberProfilePage.html\"\n",
    "file_phpbb_active = r\"C:\\<path_to>\\PAGE_ActiveTopics.html\"\n",
    "file_phpbb_timeout = r\"C:\\<path_to>\\PAGE__timeout.html\"\n",
    "file_phpbb_no_log_in = r\"C:\\<path_to>\\PAGE_NoLogin.html\"\n",
    "file_phpbb_json_orig = r\"C:\\<path_to>\\test_orig.json\"\n",
    "file_phpbb_json_copy = r\"C:\\<path_to>\\test_copy.json\"\n",
    "\n",
    "print(\"##### testing persistence module #######\")\n",
    "# testing copy and paste\n",
    "persistence.copy_file(source=file_test,target=file_test_copy,force_delete=True)\n",
    "# testing reading and writing of json file\n",
    "persistence.copy_file(source=file_phpbb_json_orig,target=file_phpbb_json_copy,force_delete=True)\n",
    "# testing json methods\n",
    "print(\"---data from json---\")\n",
    "data_from_json = persistence.read_json(file_phpbb_json_copy,debug=False)\n",
    "#data_from_json = *data_from_json\n",
    "print(len(data_from_json))\n",
    "print(\"---appending data---\")\n",
    "persistence.append_json(file_phpbb_json_copy,data_from_json,debug=True)\n",
    "print(\"---reading data again---\")\n",
    "data_from_json_2 = persistence.read_json(file_phpbb_json_copy,debug=True)\n",
    "print(data_from_json_2)\n",
    "print(\"##### testing  #######\")\n",
    "\n",
    "print(\"+++ loading test files +++\")\n",
    "soup_timeout =  soup_converter.read_soup(file_phpbb_timeout,debug=True)\n",
    "soup_active =  soup_converter.read_soup(file_phpbb_active,debug=True)\n",
    "soup_no_log_in = soup_converter.read_soup(file_phpbb_no_log_in,debug=True)\n",
    "\n",
    "print(\"+++ read soup information +++\")\n",
    "soup_posts =  soup_converter.read_soup(file_phpbb_posts,debug=False)\n",
    "post_results = soup_converter.get_posts_from_soup(soup_posts,debug=False,show_soup=False)\n",
    "print(\"Post Results read:\",len(post_results))\n",
    "soup_topics =  soup_converter.read_soup(file_phpbb_topics,debug=False)\n",
    "topic_results = soup_converter.get_topics_from_soup(soup_topics,debug=False,show_soup=False)\n",
    "print(\"Post Topics read:\",len(topic_results))\n",
    "soup_team_member =  soup_converter.read_soup(file_phpbb_team_member,debug=False)\n",
    "member_results = soup_converter.get_author_ref_from_soup(soup_team_member,debug=False,show_soup=False)\n",
    "print(\"Members Counted:\",len(member_results))\n",
    "#soup_member_profile =  soup_converter.read_soup(file_phpbb_member_profile,debug=False)\n",
    "#print(soup_member_profile)\n",
    "# ????\n",
    "member_profile = soup_converter.get_author_meta_from_soup(soup_team_member,debug=False,show_soup=True)\n",
    "print(\"Member Profile\", member_profile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (04) Testing Request Module <a name=\"04\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "from phpbb_scraper.scraper import PhpbbScraper\n",
    "from phpbb_scraper import scraper\n",
    "import phpbb_scraper.persistence as persistence\n",
    "imp.reload(scraper)\n",
    "\n",
    "# read user, password, url base from json file\n",
    "config_file = r\"C:\\<path_to>\\config.json\"\n",
    "config = persistence.read_json(filename=config_file)\n",
    "user = config['user']\n",
    "password = config['password']\n",
    "base = config['base']\n",
    "print(f\"base {base} user {user} password {password} base {base}\")\n",
    "debug = False\n",
    "wait_time = 4\n",
    "\n",
    "scraper = PhpbbScraper(base=base,debug=debug,wait_time=wait_time,user=user,password=password)\n",
    "print(f\"Scraper URL LogIn: {scraper.get_login_url()}\")\n",
    "s = scraper.get_session()\n",
    "print(f\"Session Cookies: {s.cookies}\")\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (05) Testing Web Scraping <a name=\"05\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phpbb_scraper.scraper import PhpbbScraper\n",
    "from phpbb_scraper import scraper\n",
    "from phpbb_scraper import soup_converter\n",
    "from phpbb_scraper import html_converter\n",
    "from phpbb_scraper.url_generator import PhpbbUrlGenerator\n",
    "import phpbb_scraper.persistence as persistence\n",
    "\n",
    "import traceback\n",
    "import imp\n",
    "from datetime import datetime\n",
    "imp.reload(scraper)\n",
    "imp.reload(html_converter)\n",
    "imp.reload(soup_converter)\n",
    "imp.reload(persistence)\n",
    "\n",
    "# master data / setup\n",
    "credential_file = r\"C:\\<path_to>\\credentials.json\"\n",
    "user_login_data = persistence.read_json(filename=credential_file)\n",
    "username = user_login_data['user']\n",
    "password = user_login_data['password']\n",
    "base = config['base']\n",
    "debug = False\n",
    "wait_time = 4\n",
    "\n",
    "# search term\n",
    "query = r\"offen\"\n",
    "# start index\n",
    "start = 0\n",
    "\n",
    "# construct the url\n",
    "# Hint Check Variable definition in source code\n",
    "url_gen = PhpbbUrlGenerator(base=base,debug=debug)\n",
    "\n",
    "# all defined contant values to create specific query parts\n",
    "SEARCH_PAST_DAYS = url_gen.get_query_param('SEARCH_PAST_DAYS')\n",
    "SEARCH_PAST_7DAYS = url_gen.get_query_param('SEARCH_PAST_7DAYS')\n",
    "SORT_BY_FORUM = url_gen.get_query_param('SORT_BY_FORUM')\n",
    "SORT_BY_AUTHOR = url_gen.get_query_param('SORT_BY_AUTHOR') \n",
    "SORT_BY_CREATION_DATE = url_gen.get_query_param('SORT_BY_CREATION_DATE')\n",
    "SORT_BY_TOPIC = url_gen.get_query_param('SORT_BY_TOPIC') \n",
    "SORT_BY_POST = url_gen.get_query_param('SORT_BY_POST') \n",
    "SORT_BY_RANK = url_gen.get_query_param('SORT_BY_RANK') \n",
    "SORT_BY_REGISTRATION = url_gen.get_query_param('SORT_BY_REGISTRATION') \n",
    "SORT_ASCENDING = url_gen.get_query_param('SORT_ASCENDING')\n",
    "SORT_DESCENDING = url_gen.get_query_param('SORT_DESCENDING') \n",
    "SEARCH_ALL = url_gen.get_query_param('SEARCH_ALL') \n",
    "SEARCH_TOPIC = url_gen.get_query_param('SEARCH_TOPIC')\n",
    "SEARCH_TITLE = url_gen.get_query_param('SEARCH_TITLE')\n",
    "SEARCH_FIRST_POST = url_gen.get_query_param('SEARCH_FIRST_POST')\n",
    "SEARCH_FORUM_ONLY = url_gen.get_query_param('SEARCH_FORUM_ONLY')\n",
    "SEARCH_SUBFORUM = url_gen.get_query_param('SEARCH_SUBFORUM')\n",
    "DISPLAY_TOPICS = url_gen.get_query_param('DISPLAY_TOPICS')\n",
    "DISPLAY_POSTS = url_gen.get_query_param('DISPLAY_POSTS')\n",
    "DISPLAY_NUM_CHARACTERS = url_gen.get_query_param('DISPLAY_NUM_CHARACTERS')\n",
    "KEYWORDS = url_gen.get_query_param('KEYWORDS')\n",
    "TERMS_ANY = url_gen.get_query_param('TERMS_ANY')\n",
    "TERMS_ALL = url_gen.get_query_param('TERMS_ALL')\n",
    "AUTHOR = url_gen.get_query_param('AUTHOR')\n",
    "START_POSITION = url_gen.get_query_param('START_POSITION')\n",
    "ACTIVE_TOPICS = url_gen.get_query_param('ACTIVE_TOPICS')  \n",
    "\n",
    "# search title, output list of titles\n",
    "url_search_title_desc_bydate = url_gen.get_query_url(query=query,sort_by=SORT_BY_CREATION_DATE,\n",
    "                                                     sort_order=SORT_DESCENDING,search_result=DISPLAY_TOPICS,\n",
    "                                                     terms=TERMS_ALL,search_type=SEARCH_TITLE,start=start)\n",
    "print(\"search descending by date, topics\",url_search_title_desc_bydate)\n",
    "scrape_url = url_search_title_desc_bydate\n",
    "scraper = PhpbbScraper(base=base,debug=debug,wait_time=wait_time,user=user,password=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (06) Read json file and transform to dictionary <a name=\"06\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phpbb_scraper.scraper import PhpbbScraper\n",
    "from phpbb_scraper import scraper\n",
    "from phpbb_scraper import html_converter\n",
    "from phpbb_scraper import soup_converter\n",
    "from phpbb_scraper.url_generator import PhpbbUrlGenerator\n",
    "import phpbb_scraper.persistence as persistence\n",
    "import traceback\n",
    "import imp\n",
    "import json\n",
    "from datetime import datetime\n",
    "imp.reload(scraper)\n",
    "imp.reload(html_converter)\n",
    "imp.reload(soup_converter)\n",
    "imp.reload(persistence)\n",
    "\n",
    "f = r\"C:\\<path_to>\\test.json\"\n",
    "data = persistence.read_json(f)\n",
    "print(f\"Num Entries in file {f}: {len(data)}\")\n",
    "json_string = data[0]\n",
    "\n",
    "d = persistence.read_json_as_dict(filename=f,debug=True)\n",
    "print(d[1][\"Thema\"][\"link\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (07) Create HTML Table  and save it to file <a name=\"07\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "from phpbb_scraper import persistence\n",
    "from phpbb_scraper import html_converter\n",
    "imp.reload(persistence)\n",
    "imp.reload(html_converter)\n",
    "data = \"string2\"\n",
    "path = r\"C:<path_to>\"\n",
    "filename = r\"test\"\n",
    "file_ext = \"txt\"\n",
    "append_tst = True\n",
    "#persistence.save_data(data,filename,path=path,file_extension=file_ext,append_timestamp=True)\n",
    "\n",
    "#a=[\"test\"]\n",
    "#type(a) is list\n",
    "url = \"http://test_page\"\n",
    "linktext = \"link\"\n",
    "link = html_converter.link(link=url,text=linktext)\n",
    "test_table = [[\"a\",link],[\"c\",link],[\"e\",link]]\n",
    "              \n",
    "test_html = html_converter.get_html_table(test_table)\n",
    "persistence.save_data(test_html,filename,path=path,file_extension=\"html\",append_timestamp=True)\n",
    "print(test_html)\n",
    "               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (08) Read HTML file as soup, create filename, save as json, read json as dictionary, convert to HTML table and save HTML file <a name=\"08\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read html file with posts and transform it as json file and back as dictionary\n",
    "from phpbb_scraper.scraper import PhpbbScraper\n",
    "from phpbb_scraper import scraper\n",
    "from phpbb_scraper import html_converter\n",
    "from phpbb_scraper import soup_converter\n",
    "from phpbb_scraper.url_generator import PhpbbUrlGenerator\n",
    "import phpbb_scraper.persistence as persistence\n",
    "import traceback\n",
    "import imp\n",
    "import json\n",
    "from datetime import datetime\n",
    "imp.reload(scraper)\n",
    "imp.reload(html_converter)\n",
    "imp.reload(soup_converter)\n",
    "imp.reload(persistence)\n",
    "\n",
    "p_html = r\"C:\\<path_to>\\test_posts.html\"\n",
    "p_soup = soup_converter.read_soup(url=p_html)\n",
    "p = soup_converter.get_posts_from_soup(p_soup)\n",
    "#print(f\"type {type(p[0])} {p[0]}\")\n",
    "print(\"----------------------------------\")\n",
    "t_html = r\"C:\\<path_to>\\test_topics.html\"\n",
    "t_soup = soup_converter.read_soup(url=t_html)\n",
    "t = soup_converter.get_topics_from_soup(t_soup)\n",
    "#print(f\"type {type(t[0])} {t[0]}\")\n",
    "print(\"----------------------------------\")\n",
    "d = r\"C:\\<path_to>\\Desktop\"\n",
    "f = r\"test_posts\"\n",
    "filename = persistence.create_filename(f,path=d,file_extension=\"json\",append_timestamp=False)\n",
    "print(filename)\n",
    "persistence.append_json(filename=filename,dict_entries=p)\n",
    "print(f\"done, written on file {filename}\")\n",
    "f = r\"test_topics\"\n",
    "filename = persistence.create_filename(f,path=d,file_extension=\"json\",append_timestamp=False)\n",
    "persistence.append_json(filename=filename,dict_entries=t)\n",
    "print(f\"done, written on file {filename}\")\n",
    "f_p = r\"C:\\<path_to>\\test_posts.json\"\n",
    "f_t = r\"C:\\<path_to>\\test_topics.json\"\n",
    "#data = persistence.read_json(f)\n",
    "data_t  = persistence.read_json_as_dict(f_t)\n",
    "html_t = html_converter.dict_as_html(data_t)\n",
    "\n",
    "\n",
    "print(f\"TOPICS type {type(data_t[0])} \\n {data_t[0]}\")\n",
    "print(f\"Num Entries in file {f}: {len(data_t)}\")\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "data_p  = persistence.read_json_as_dict(f_p)\n",
    "html_p = html_converter.dict_as_html(data_p)\n",
    "print(f\"POSTS type {type(data_p[0])} \\n {data_p[0]}\")\n",
    "print(f\"Num Entries in file {f}: {len(data_p)}\")\n",
    "####################\n",
    "#def save_data(data,filename,path=None,file_extension=None,append_timestamp=True):\n",
    "ft = r\"test_topics\"\n",
    "fp = r\"test_posts\"\n",
    "e = \"html\"\n",
    "persistence.save_data(html_p,fp,path=d,file_extension=e,append_timestamp=False)\n",
    "persistence.save_data(html_t,ft,path=d,file_extension=e,append_timestamp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (09) Read Credentials and URL Base from Config file, initialize session and scraper, read one page, save data   <a name=\"09\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phpbb_scraper.scraper import PhpbbScraper\n",
    "from phpbb_scraper import scraper\n",
    "from phpbb_scraper import soup_converter\n",
    "from phpbb_scraper import html_converter\n",
    "from phpbb_scraper.url_generator import PhpbbUrlGenerator\n",
    "import phpbb_scraper.persistence as persistence\n",
    "\n",
    "import traceback\n",
    "import imp\n",
    "from datetime import datetime\n",
    "imp.reload(scraper)\n",
    "imp.reload(html_converter)\n",
    "imp.reload(soup_converter)\n",
    "imp.reload(persistence)\n",
    "\n",
    "# read user, password, url base from json file\n",
    "config_file = r\"C:\\3<path_to>\\config.json\"\n",
    "config = persistence.read_json(filename=config_file)\n",
    "user = config['user']\n",
    "password = config['password']\n",
    "base = config['base']\n",
    "debug = False\n",
    "wait_time = 5\n",
    "scraper = PhpbbScraper(base=base,debug=debug,wait_time=wait_time,user=user,password=password)\n",
    "#print(f\"base {base} user {user} password {password} base {base}\")\n",
    "\n",
    "url = r\"https://<forum>\"\n",
    "print(url)\n",
    "p = r\"C:\\<path_to>\\Desktop\"\n",
    "fu = r\"test_utf8\"\n",
    "f8 = r\"test_8859\"\n",
    "iso_encode = 'iso-8859-1'\n",
    "rf = r\"C:\\<path_to>\\test_utf8.html\"\n",
    "try:\n",
    "    sess = scraper.get_session()\n",
    "    soup = scraper.get_data(session=sess,url=url)    \n",
    "#def save_data(data,filename,path=None,file_extension=None,append_timestamp=True,encoding='utf-8'):    \n",
    "    persistence.save_data(str(soup),fu,path=p,file_extension=\"html\",append_timestamp=False)\n",
    "    #persistence.save_data(str(soup),f8,path=p,file_extension=\"html\",append_timestamp=False,encoding=iso_encode)\n",
    "    #print(soup)\n",
    "    sess.close()    \n",
    "except:    \n",
    "    print(traceback.format_exc())\n",
    "soup_read = soup_converter.read_soup(rf)    \n",
    "print(soup_read)\n",
    "print(\"end\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (10) Read HTML file names from directory, read single html file <a name=\"10\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = r\"C:\\<path_to>\"\n",
    "file_name =  persistence.read_html_file_names(d)[0]\n",
    "print(file_name)\n",
    "soup_read = soup_converter.read_soup(file_name)\n",
    "print(soup_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (11) Generate Query URL, create URL iterator to call URLs with indices <a name=\"11\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phpbb_scraper.scraper import PhpbbScraper\n",
    "from phpbb_scraper import scraper\n",
    "from phpbb_scraper import soup_converter\n",
    "from phpbb_scraper import html_converter\n",
    "from phpbb_scraper.url_generator import PhpbbUrlGenerator\n",
    "import phpbb_scraper.persistence as persistence\n",
    "\n",
    "import traceback\n",
    "import imp\n",
    "from datetime import datetime\n",
    "imp.reload(scraper)\n",
    "imp.reload(html_converter)\n",
    "imp.reload(soup_converter)\n",
    "imp.reload(persistence)\n",
    "\n",
    "start_index = 0\n",
    "increment = 70\n",
    "num_steps = 4\n",
    "end_index = start_index + num_steps*increment\n",
    "#print(end_index)\n",
    "r = list(range(start_index,end_index,increment))\n",
    "\n",
    "url_gen = PhpbbUrlGenerator(base=\"http://<forum>\",debug=False)\n",
    "\n",
    "# Hint Check Variable definition in source code\n",
    "sort_by_creation_date = url_gen.get_query_param('SORT_BY_CREATION_DATE')\n",
    "sort_by_forum = url_gen.get_query_param('SORT_BY_FORUM')\n",
    "sort_descending = url_gen.get_query_param('SORT_DESCENDING')\n",
    "sort_by_rank = url_gen.get_query_param('SORT_BY_RANK')\n",
    "display_posts = url_gen.get_query_param('DISPLAY_POSTS')\n",
    "\n",
    "#print(\"Possible Query Parameters and url params\",url_gen.QUERY_PARAMS_DICT)\n",
    "#print(\"----------------------\")\n",
    "\n",
    "start = 20\n",
    "query = \"offen test\"\n",
    "\n",
    "search_posts_desc_bydate = url_gen.get_query_url(query=query,sort_by=sort_by_creation_date,\n",
    "                                             sort_order=sort_descending,search_result=display_posts,start=0)\n",
    "\n",
    "query_it = url_gen.get_query_url_iterator(query=query,sort_by=sort_by_creation_date,\n",
    "                                          sort_order=sort_descending,search_result=display_posts,\n",
    "                                          start=15,num_steps=3,increment=15)\n",
    "\n",
    "for query in query_it:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (12) Read Configuration, execute query, copy result to soup, replace relative by absolute links in soup <a name=\"12\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phpbb_scraper.url_generator import PhpbbUrlGenerator\n",
    "from phpbb_scraper import scraper\n",
    "from phpbb_scraper.scraper import ScraperExecutor\n",
    "from phpbb_scraper import soup_converter\n",
    "import imp\n",
    "#PhpbbUrlGenerator.QUERY_PARAMS_DICT\n",
    "imp.reload(scraper)\n",
    "config_file = r\"C:\\<path_to>\\config.json\"\n",
    "target_dir = r\"C:\\<path_to>\\TEST\"\n",
    "executor = ScraperExecutor(config_file=config_file,target_dir=target_dir)\n",
    "base_url = executor.base_url\n",
    "DISPLAY_TOPICS = executor.QUERIES['DISPLAY_TOPICS']\n",
    "url_gen = PhpbbUrlGenerator(base=base_url)\n",
    "#print(executor.QUERIES)\n",
    "# build url for latest posts\n",
    "#   def get_query_url(cls,query=None,base=None,sort_by=None,sort_order=None,\n",
    "#                      search_type=None,search_forum=None,search_result=None,num_chars=None,\n",
    "#                      terms=None,author=None,start=None,member_list=False,active_topics=False):\n",
    "url_latest_topics = url_gen.get_query_url(active_topics=True)\n",
    "print(url_latest_topics)\n",
    "soup = executor.get_soup(url_latest_topics)\n",
    "print(f\"soup was read, length {len(str(soup))}\")\n",
    "# replace relative links by absolute path\n",
    "soup = soup_converter.replace_relative_links(soup,base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (13) Create Filename, Save soup as data string <a name=\"13\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phpbb_scraper import persistence\n",
    "import imp\n",
    "imp.reload(persistence)\n",
    "name = r\"test_active_topics\"\n",
    "path = r\"C:\\<path_to>\\TEST\"\n",
    "extension = \"html\"\n",
    "append_timestamp = False\n",
    "filename = persistence.create_filename(name,path=path,file_extension=extension,append_timestamp=append_timestamp)\n",
    "print(filename)\n",
    "persistence.save_data(str(soup),filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (14) Convert soup as json entries, save it to file <a name=\"14\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phpbb_scraper import soup_converter\n",
    "from phpbb_scraper import persistence\n",
    "import imp\n",
    "imp.reload(persistence)\n",
    "imp.reload(soup_converter)\n",
    "#print(soup)\n",
    "#soup_converter.get_search_result_data_from_soup(soup=soup,show_soup=True)\n",
    "#soup\n",
    "json_entries=soup_converter.get_topics_from_soup(soup,debug=False)\n",
    "print((json_entries[0]))\n",
    "name = r\"test\"\n",
    "path = r\"C:\\<path_to>\\TEST\"\n",
    "extension = \"json\"\n",
    "append_timestamp = True\n",
    "filename = persistence.create_filename(name,path=path,file_extension=extension,append_timestamp=append_timestamp)\n",
    "print(filename)\n",
    "#def create_filename(filename,path=None,file_extension=None,append_timestamp=True):\n",
    "#str(soup)\n",
    "persistence.append_json(filename, json_entries, debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (15) Read Snippet for Number of Topics in Soup <a name=\"15\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phpbb_scraper import soup_converter\n",
    "from phpbb_scraper import persistence\n",
    "import re\n",
    "imp.reload(persistence)\n",
    "imp.reload(soup_converter)\n",
    "testfile = r\"C:\\<path_to>\\test_active_topics.html\"\n",
    "base = \"###LINK###\"\n",
    "# def get_search_result_data_from_soup(soup, debug=False, show_soup=False) persistence\n",
    "soup = soup_converter.read_soup(testfile,base_url=base,debug=True)\n",
    "#soup_converter.get_search_result_data_from_soup(soup,show_soup=True)\n",
    "#print(str(soup))\n",
    "soup_converter.get_num_topics(soup)\n",
    "# try:\n",
    "#     num = int(re.search('Die Suche ergab (\\d+) Treffer', str(soup)).group(1))\n",
    "# except:\n",
    "#     num = 0\n",
    "# print(num) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (16) Read JSON file, convert to dictionary, export as HTML file <a name=\"16\"></a>\n",
    "[BACK](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "from phpbb_scraper import persistence \n",
    "from phpbb_scraper import html_converter\n",
    "#read json file\n",
    "imp.reload(persistence)\n",
    "json_file = r\"C:\\<path_to>\\test.json\"\n",
    "html_file = r\"C:\\<path_to>\\test_html.html\"\n",
    "json_data = persistence.read_json(json_file, debug=False)\n",
    "print(json_data[0])\n",
    "# transform each json entry\n",
    "\"\"\"  {\n",
    "        \"<key>\": \"<value>\",\n",
    "        \"link\": \"<link>\"\n",
    "       },\n",
    "       into dictionary entry\n",
    "       {<key>:{\"value\":<value>,\"link\":<link>}}\n",
    "\"\"\"\n",
    "json_as_dict = persistence.read_json_as_dict(json_file,debug=False)\n",
    "#print(len(json_as_dict))\n",
    "print(\"Sample Entry\",json_as_dict[0])\n",
    "print(\"keys\",json_as_dict[0].keys())\n",
    "#sample readout\n",
    "key = \"Thema\"\n",
    "dict_data = json_as_dict[0][key]\n",
    "#print(f\" key:{key},value {dict_data[\"value\"]},link {dict_data[\"link\"]}\")\n",
    "print(f\"\\n ( key:{key} \\n value {dict_data['value']} \\n link {dict_data['link']} )\")\n",
    "# export as HTML Table and Save\n",
    "html = html_converter.dict_as_html(json_as_dict)\n",
    "persistence.save_data(html,html_file)\n",
    "print(\"file saved\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
